{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6czvz5VKO5M"
   },
   "source": [
    "# Notebook for Programming in Problem 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5o8HI5JqTvU5"
   },
   "source": [
    "## Learning Objectives\n",
    "In this problem, we will use [PyTorch](https://pytorch.org/) to implement long short-term memory (LSTM) for named entity recognition (NER). We will use the provided dataset 'eng.train' and 'eng.val'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ObrHyvWvTyGZ"
   },
   "source": [
    "## Writing Code\n",
    "Look for the keyword \"TODO\" and fill in your code in the empty space.\n",
    "Feel free to change function signatures, but be careful that you might need to also change how they are called in other parts of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "r6YTnpgbFdMI"
   },
   "outputs": [],
   "source": [
    "#!nvidia-smi # you may need to try reconnecting to get a T4 gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnYMKJlKNXYe"
   },
   "source": [
    "## Installing PyTorch and Other Packages\n",
    "\n",
    "Install PyTorch using pip. See [https://pytorch.org/](https://pytorch.org/) if you want to install it on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-dRVuiP_JVdT"
   },
   "outputs": [],
   "source": [
    "#!pip install torch torchtext -f https://download.pytorch.org/whl/torch_stable.html\n",
    "#!pip install torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPsFH637OpLy"
   },
   "source": [
    "Test if our installation works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "c62StNb2NvKk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch successfully installed!\n",
      "Version: 2.2.2+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Multiply two matrices on GPU\n",
    "a = torch.rand(100, 200).cuda()\n",
    "b = torch.rand(200, 100).cuda()\n",
    "c = torch.matmul(a, b)\n",
    "\n",
    "print(\"PyTorch successfully installed!\")\n",
    "print(\"Version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qaC8sxcqkGX"
   },
   "source": [
    "Also install [scikit-learn](https://scikit-learn.org/stable/). We will use it for calculating evaluation metrics such as accuracy and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "i5Y2xB_uqqM9"
   },
   "outputs": [],
   "source": [
    "#!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhV4CYivRbt4"
   },
   "source": [
    "Let's import all the packages at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "EjRM4cCFRh-d"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.vocab import Vocab, vocab\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Dict, Optional, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yn1bIPjAN-9V"
   },
   "source": [
    "## Long Short Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJOKIneRTrTH"
   },
   "source": [
    "### Data Loading\n",
    "\n",
    "We will use the provided datasets for named entity recognition ('eng.train' & 'eng.val'). Take a look at the first 50 lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "lWqz7kDxSqeb"
   },
   "outputs": [],
   "source": [
    "#!cat eng.train | head -n 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVt1a6nzWsiF"
   },
   "source": [
    "Each line corresponds to a word. Different sentences are separated by an additional line break. Take \"EU NNP I-NP ORG\" as an example. \"EU\" is a word. \"NNP\" and \"I-NP\" are tags for POS tagging and chunking, which we will ignore. \"ORG\" is the tag for NER, which is our prediction target. There are 5 possible values for the NER tag: ORG, PER, LOC, MISC, and O.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "WnNfOBUYJvVW"
   },
   "outputs": [],
   "source": [
    "# A sentence is a list of (word, tag) tuples.\n",
    "# For example, [(\"hello\", \"O\"), (\"world\", \"O\"), (\"!\", \"O\")]\n",
    "Sentence = List[Tuple[str, str]]\n",
    "\n",
    "\n",
    "def read_data_file(\n",
    "    datapath: str,\n",
    ") -> Tuple[List[Sentence], Dict[str, int], Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Read and preprocess input data from the file `datapath`.\n",
    "    Example:\n",
    "    ```\n",
    "        sentences, word_cnt, tag_cnt = read_data_file(\"eng.train\")\n",
    "    ```\n",
    "    Return values:\n",
    "        `sentences`: a list of sentences, including words and NER tags\n",
    "        `word_cnt`: a Counter object, the number of occurrences of each word\n",
    "        `tag_cnt`: a Counter object, the number of occurences of each NER tag\n",
    "    \"\"\"\n",
    "    sentences: List[Sentence] = []\n",
    "    word_cnt: Dict[str, int] = Counter()\n",
    "    tag_cnt: Dict[str, int] = Counter()\n",
    "\n",
    "    for sentence_txt in open(datapath).read().split(\"\\n\\n\"):\n",
    "        if \"DOCSTART\" in sentence_txt:\n",
    "            # Ignore dummy sentences at the begining of each document.\n",
    "            continue\n",
    "        # Read a new sentence\n",
    "        sentences.append([])\n",
    "        for token in sentence_txt.split(\"\\n\"):\n",
    "            w, _, _, t = token.split()\n",
    "            # Replace all digits with \"0\" to reduce out-of-vocabulary words\n",
    "            w = re.sub(\"\\d\", \"0\", w)\n",
    "            word_cnt[w] += 1\n",
    "            tag_cnt[t] += 1\n",
    "            sentences[-1].append((w, t))\n",
    "\n",
    "    return sentences, word_cnt, tag_cnt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "WLMGYSZ7KxzP"
   },
   "outputs": [],
   "source": [
    "# Some helper code\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\"\n",
    "    Use GPU when it is available; use CPU otherwise.\n",
    "    See https://pytorch.org/docs/stable/notes/cuda.html#device-agnostic-code\n",
    "    \"\"\"\n",
    "    return torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wVHAOb7iMPwC"
   },
   "outputs": [],
   "source": [
    "def eval_metrics(ground_truth: List[int], predictions: List[int]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Calculate various evaluation metrics such as accuracy and F1 score\n",
    "    Parameters:\n",
    "        `ground_truth`: the list of ground truth NER tags\n",
    "        `predictions`: the list of predicted NER tags\n",
    "    \"\"\"\n",
    "    f1_scores = f1_score(ground_truth, predictions, average=None)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(ground_truth, predictions),\n",
    "        \"f1\": f1_scores,\n",
    "        \"average f1\": np.mean(f1_scores),\n",
    "        \"confusion matrix\": confusion_matrix(ground_truth, predictions),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7s830dhbnj1L"
   },
   "source": [
    "## Long Short-term Memory (LSTM)\n",
    "\n",
    "Now we implement an one-layer LSTM for the NER task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "to7DnWNiY5ZS"
   },
   "source": [
    "### Data Loading **(4 points)**\n",
    "\n",
    "We first implement the data loader. In the provided datasets, each data example is a variable-length sentence. How can we pack multiple sentences with different lengths into the same batch? One possible solution is to pad them to the same length using a special token. The code below illustrates the idea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "J5oVgqE7JaJp"
   },
   "outputs": [],
   "source": [
    "# 3 sentences with different lengths\n",
    "sentence_1 = torch.tensor([6, 1, 2])\n",
    "sentence_2 = torch.tensor([4, 2, 7, 7, 9])\n",
    "sentence_3 = torch.tensor([3, 4])\n",
    "# Form a batch by padding 0\n",
    "sentence_batch = torch.tensor([\n",
    "    [6, 1, 2, 0, 0],\n",
    "    [4, 2, 7, 7, 9],\n",
    "    [3, 4, 0, 0, 0],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udC0SMjkKaCN"
   },
   "source": [
    "We implement the above idea in a customized batching function `form_batch`. Optionally, see [here](https://pytorch.org/docs/stable/data.html#loading-batched-and-non-batched-data) for how batching works in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "sACcGN4XYMgj"
   },
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Each data example is a sentence, including its words and NER tags.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, datapath: str, words_vocab: Optional[Vocab] = None, tags_vocab: Optional[Vocab] = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the dataset by reading from datapath.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.sentences: List[Sentence] = []\n",
    "        UNKNOWN = \"<UNKNOWN>\"\n",
    "        PAD = \"<PAD>\"  # Special token used for padding\n",
    "\n",
    "        print(\"Loading data from %s\" % datapath)\n",
    "        self.sentences, word_cnt, tag_cnt = read_data_file(datapath)\n",
    "        print(\"%d sentences loaded.\" % len(self.sentences))\n",
    "\n",
    "        if words_vocab is None:\n",
    "            words_vocab = vocab(word_cnt, specials=[PAD, UNKNOWN])\n",
    "            words_vocab.set_default_index(words_vocab[UNKNOWN])\n",
    "\n",
    "        self.words_vocab = words_vocab\n",
    "\n",
    "        self.unknown_idx = self.words_vocab[UNKNOWN]\n",
    "        self.pad_idx = self.words_vocab[PAD]\n",
    "\n",
    "        if tags_vocab is None:\n",
    "            tags_vocab = vocab(tag_cnt, specials=[])\n",
    "        self.tags_vocab = tags_vocab\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Sentence:\n",
    "        \"\"\"\n",
    "        Get the idx'th sentence in the dataset.\n",
    "        \"\"\"\n",
    "        return self.sentences[idx]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Return the number of sentences in the dataset.\n",
    "        \"\"\"\n",
    "        # TODO: Implement this method\n",
    "        # START HERE\n",
    "        return len(self.sentences)\n",
    "        # END\n",
    "\n",
    "    def form_batch(self, sentences: List[Sentence]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        A customized function for batching a number of sentences together.\n",
    "        Different sentences have different lengths. Let max_len be the longest length.\n",
    "        When packing them into one tensor, we need to pad all sentences to max_len.\n",
    "        Return values:\n",
    "            `words`: a list in which each element itself is a list of words in a sentence\n",
    "            `word_idxs`: a batch_size x max_len tensor.\n",
    "                       word_idxs[i][j] is the index of the j'th word in the i'th sentence .\n",
    "            `tags`: a list in which each element itself is a list of tags in a sentence\n",
    "            `tag_idxs`: a batch_size x max_len tensor\n",
    "                      tag_idxs[i][j] is the index of the j'th tag in the i'th sentence.\n",
    "            `valid_mask`: a batch_size x max_len tensor\n",
    "                        valid_mask[i][j] is True if the i'th sentence has the j'th word.\n",
    "                        Otherwise, valid[i][j] is False.\n",
    "        \"\"\"\n",
    "        words: List[List[str]] = []\n",
    "        tags: List[List[str]] = []\n",
    "        max_len = -1  # length of the longest sentence\n",
    "        for sent in sentences:\n",
    "            words.append([])\n",
    "            tags.append([])\n",
    "            for w, t in sent:\n",
    "                words[-1].append(w)\n",
    "                tags[-1].append(t)\n",
    "            max_len = max(max_len, len(words[-1]))\n",
    "\n",
    "        batch_size = len(sentences)\n",
    "        word_idxs = torch.full(\n",
    "            (batch_size, max_len), fill_value=self.pad_idx, dtype=torch.int64\n",
    "        )\n",
    "        tag_idxs = torch.full_like(word_idxs, fill_value=self.tags_vocab[\"O\"])\n",
    "        valid_mask = torch.zeros_like(word_idxs, dtype=torch.bool)\n",
    "\n",
    "        ## TODO: Fill in the values in word_idxs, tag_idxs, and valid_mask\n",
    "        ## Caveat: There may be out-of-vocabulary words in validation data\n",
    "        ## See torchtext.vocab.Vocab: https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab\n",
    "        ## START HERE\n",
    "        for b in range(batch_size) : \n",
    "            l_b = len(sentences[b])\n",
    "            word_idxs[b][:l_b] =  torch.tensor(self.words_vocab.lookup_indices([item[0] for item in sentences[b]]))\n",
    "            tag_idxs[b][:l_b]  = torch.tensor(self.tags_vocab.lookup_indices([item[1] for item in sentences[b]]))\n",
    "            valid_mask[b][:l_b] = True    \n",
    "        # END\n",
    "\n",
    "        return {\n",
    "            \"words\": words,\n",
    "            \"word_idxs\": word_idxs,\n",
    "            \"tags\": tags,\n",
    "            \"tag_idxs\": tag_idxs,\n",
    "            \"valid_mask\": valid_mask,\n",
    "        }\n",
    "\n",
    "def create_sequence_dataloaders(\n",
    "    batch_size: int, shuffle: bool = True\n",
    ") -> Tuple[DataLoader, DataLoader, Vocab]:\n",
    "    \"\"\"\n",
    "    Create the dataloaders for training and validaiton.\n",
    "    \"\"\"\n",
    "    ds_train = SequenceDataset(\"eng.train\")\n",
    "    ds_val = SequenceDataset(\"eng.val\", words_vocab=ds_train.words_vocab, tags_vocab=ds_train.tags_vocab)\n",
    "    loader_train = DataLoader(\n",
    "        ds_train,\n",
    "        batch_size,\n",
    "        shuffle,\n",
    "        collate_fn=ds_train.form_batch,  # customized function for batching\n",
    "        drop_last=True,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    loader_val = DataLoader(\n",
    "        ds_val, batch_size, collate_fn=ds_val.form_batch, pin_memory=True\n",
    "    )\n",
    "    return loader_train, loader_val, ds_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2EcVxYuYvGv"
   },
   "source": [
    "Here is a simple sanity-check. Try to understand its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "TazmodGWYx2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from eng.train\n",
      "14041 sentences loaded.\n",
      "Loading data from eng.val\n",
      "3490 sentences loaded.\n",
      "Iterating on the training data..\n",
      "{'words': [['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], ['Peter', 'Blackburn'], ['BRUSSELS', '0000-00-00']], 'word_idxs': tensor([[ 2,  3,  4,  5,  6,  7,  8,  9, 10],\n",
      "        [11, 12,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [13, 14,  0,  0,  0,  0,  0,  0,  0]]), 'tags': [['ORG', 'O', 'MISC', 'O', 'O', 'O', 'MISC', 'O', 'O'], ['PER', 'PER'], ['LOC', 'O']], 'tag_idxs': tensor([[0, 1, 2, 1, 1, 1, 2, 1, 1],\n",
      "        [3, 3, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [4, 1, 1, 1, 1, 1, 1, 1, 1]]), 'valid_mask': tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False, False, False, False]])}\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "def check_sequence_dataloader() -> None:\n",
    "    loader_train, _, _ = create_sequence_dataloaders(batch_size=3, shuffle=False)\n",
    "    print(\"Iterating on the training data..\")\n",
    "    for i, data_batch in enumerate(loader_train):\n",
    "        if i == 0:\n",
    "            print(data_batch)\n",
    "    print(\"Done!\")\n",
    "\n",
    "check_sequence_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifk3i-obY8YB"
   },
   "source": [
    "### Implement the Model **(8 points)**\n",
    "\n",
    "Next, implement LSTM for predicting NER tags from input words. [nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM) is definitely useful. Further, it is tricky to handle sentences in the same batch with different lengths. Please read the PyTorch documentation in detail!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "3V0NvQynZF8e"
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Long short-term memory for NER\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, words_vocab: Vocab, tags_vocab:Vocab, d_emb: int, d_hidden: int, bidirectional: bool) -> None:\n",
    "        \"\"\"\n",
    "        Initialize an LSTM\n",
    "        Parameters:\n",
    "            `words_vocab`: vocabulary of words\n",
    "            `tags_vocab`: vocabulary of tags\n",
    "            `d_emb`: dimension of word embeddings (D)\n",
    "            `d_hidden`: dimension of the hidden layer (H)\n",
    "            `bidirectional`: true if LSTM should be bidirectional\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Create the word embeddings (nn.Embedding),\n",
    "        #       the LSTM (nn.LSTM) and the output layer (nn.Linear).\n",
    "        #       Read the torch docs for additional guidance : https://pytorch.org/docs/stable\n",
    "        #       Note: Pay attention to the LSTM output shapes!\n",
    "        # START HERE\n",
    "        self.E = nn.Embedding(len(words_vocab),d_emb)\n",
    "        self.R = nn.LSTM(d_emb,d_hidden, batch_first=True,bidirectional = bidirectional ,num_layers = 2)\n",
    "        self.act = nn.ReLU()\n",
    "        #self.norm = nn.InstanceNorm1d(d_hidden*2)\n",
    "        \n",
    "        if bidirectional :\n",
    "            self.L = nn.Linear(d_hidden*2,len(tags_vocab))\n",
    "            self.norm = nn.BatchNorm1d(d_hidden*2)\n",
    "        else : \n",
    "            self.L = nn.Linear(d_hidden,len(tags_vocab))\n",
    "            self.norm = nn.BatchNorm1d(d_hidden)\n",
    "        self.S = nn.Softmax(dim=2)\n",
    "        # END\n",
    "\n",
    "    def forward(\n",
    "        self, word_idxs: torch.Tensor, valid_mask: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Given words in sentences, predict the logits of the NER tag.\n",
    "        Parameters:\n",
    "            `word_idxs`: a batch_size x max_len tensor\n",
    "            `valid_mask`: a batch_size x max_len tensor\n",
    "        Return values:\n",
    "            `logits`: a batch_size x max_len x 5 tensor\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass\n",
    "        # START HERE\n",
    "        valid_mask = torch.unsqueeze(valid_mask,-1)\n",
    "        #print(word_idxs.shape)\n",
    "        z = self.E(word_idxs)\n",
    "        #print(\"emb : {}\".format(z.shape))\n",
    "        z,h = self.R(z)\n",
    "        z = torch.permute(z,(0,2,1))\n",
    "        z = self.norm(z)\n",
    "        z = torch.permute(z,(0,2,1))\n",
    "        z = self.act(z)\n",
    "        #print(\"z : {}\".format(z.shape))\n",
    "        z = self.L(z)\n",
    "        z = self.act(z)\n",
    "        #print(\"z : {}\".format(z.shape))\n",
    "        #print(valid_mask.shape)\n",
    "        logits = self.S(z)*valid_mask\n",
    "\n",
    "        # END\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BFTKaB4Zydx"
   },
   "source": [
    "We do a sanity-check by loading a batch of data examples and pass it through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "PKg1ni4QZ6D1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from eng.train\n",
      "14041 sentences loaded.\n",
      "Loading data from eng.val\n",
      "3490 sentences loaded.\n",
      "LSTM(\n",
      "  (E): Embedding(20102, 64)\n",
      "  (R): LSTM(64, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (act): ReLU()\n",
      "  (L): Linear(in_features=256, out_features=5, bias=True)\n",
      "  (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (S): Softmax(dim=2)\n",
      ")\n",
      "Input word_idxs shape: torch.Size([4, 19])\n",
      "Input valid_mask shape: torch.Size([4, 19])\n",
      "Output logits shape: torch.Size([4, 19, 5])\n"
     ]
    }
   ],
   "source": [
    "def check_lstm() -> None:\n",
    "    # Hyperparameters\n",
    "    batch_size = 4\n",
    "    d_emb = 64\n",
    "    d_hidden = 128\n",
    "    bidirectional = True\n",
    "    # Create the dataloaders and the model\n",
    "    loader_train, _, ds_train = create_sequence_dataloaders(batch_size)\n",
    "    model = LSTM(ds_train.words_vocab, ds_train.tags_vocab, d_emb, d_hidden, bidirectional)\n",
    "    device = get_device()\n",
    "    model.to(device)\n",
    "    print(model)\n",
    "    # Get the first batch\n",
    "    data_batch = next(iter(loader_train))\n",
    "    # Move data to GPU\n",
    "    word_idxs = data_batch[\"word_idxs\"].to(device, non_blocking=True)\n",
    "    tag_idxs = data_batch[\"tag_idxs\"].to(device, non_blocking=True)\n",
    "    valid_mask = data_batch[\"valid_mask\"].to(device, non_blocking=True)\n",
    "    # Calculate the model\n",
    "    print(\"Input word_idxs shape:\", word_idxs.size())\n",
    "    print(\"Input valid_mask shape:\", valid_mask.size())\n",
    "    logits = model(word_idxs, valid_mask)\n",
    "    print(\"Output logits shape:\", logits.size())\n",
    "\n",
    "\n",
    "check_lstm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jddDYUiLY-hc"
   },
   "source": [
    "### Training and Validation **(6 points)**\n",
    "\n",
    "Complete the functions for training and validating the LSTM model. When calculating the loss function, you only want to include values from valid positions (where `valid_mask` is `True`). The `reduction` parameter in [F.cross_entropy](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.cross_entropy) may be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "hv_15mnXZ_dy"
   },
   "outputs": [],
   "source": [
    "def train_lstm(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    optimizer: optim.Optimizer,\n",
    "    device: torch.device,\n",
    "    silent: bool = False,  # whether to print the training loss\n",
    ") -> Tuple[float, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Train the LSTM model.\n",
    "    Return values:\n",
    "        1. the average training loss\n",
    "        2. training metrics such as accuracy and F1 score\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    ground_truth = []\n",
    "    predictions = []\n",
    "    losses = []\n",
    "    report_interval = 100\n",
    "\n",
    "    for i, data_batch in enumerate(loader):\n",
    "        word_idxs = data_batch[\"word_idxs\"].to(device, non_blocking=True)\n",
    "        tag_idxs = data_batch[\"tag_idxs\"].to(device, non_blocking=True)\n",
    "        valid_mask = data_batch[\"valid_mask\"].to(device, non_blocking=True)\n",
    "\n",
    "        # TODO:\n",
    "        # 1. Perform the forward pass to calculate the model's output. Save it to the variable \"logits\".\n",
    "        # 2. Calculate the loss using the output and the ground truth tags. Save it to the variable \"loss\".\n",
    "        # 3. Perform the backward pass to calculate the gradient.\n",
    "        # 4. Use the optimizer to update model parameters.\n",
    "        # Caveat: \n",
    "        # 1. You may need to call optimizer.zero_grad(). Figure out what it does! \n",
    "        # 2. When calculating the loss, you should only consider positions where valid_mask == True\n",
    "        # START HERE\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "       \n",
    "        logits = model(word_idxs,valid_mask)\n",
    "        gt = F.one_hot(tag_idxs,num_classes=logits.shape[-1])\n",
    "        loss = torch.sum(torch.abs(gt-logits))\n",
    "        # END\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # we get (unmasked) predictions by getting argmax of logits along last dimension (You will need to define logits!)\n",
    "        net_predictions = torch.argmax(logits, -1)\n",
    "\n",
    "        # flattening a tensor simply converts it from a multi-dimensional to a single-dimensional tensor; we flatten here to make it easier to extract ground truths and predictions\n",
    "        tag_idxs_flat = tag_idxs.flatten()\n",
    "        valid_mask_flat = valid_mask.flatten()\n",
    "        net_predictions_flat = net_predictions.flatten()\n",
    "\n",
    "        ground_truth.extend(tag_idxs_flat[valid_mask_flat].tolist())\n",
    "        predictions.extend(net_predictions_flat[valid_mask_flat].tolist())\n",
    "\n",
    "        if not silent and i > 0 and i % report_interval == 0:\n",
    "            print(\n",
    "                \"\\t[%06d/%06d] Loss: %f\"\n",
    "                % (i, len(loader), np.mean(losses[-report_interval:]))\n",
    "            )\n",
    "\n",
    "    return np.mean(losses), eval_metrics(ground_truth, predictions)\n",
    "\n",
    "\n",
    "def validate_lstm(\n",
    "    model: nn.Module, loader: DataLoader, device: torch.device\n",
    ") -> Tuple[float, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Validate the model.\n",
    "    Return the validation loss and metrics.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    ground_truth = []\n",
    "    predictions = []\n",
    "    losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for data_batch in loader:\n",
    "            word_idxs = data_batch[\"word_idxs\"].to(device, non_blocking=True)\n",
    "            tag_idxs = data_batch[\"tag_idxs\"].to(device, non_blocking=True)\n",
    "            valid_mask = data_batch[\"valid_mask\"].to(device, non_blocking=True)\n",
    "\n",
    "            # TODO: Similar to what you did in train_lstm, but only step 1 and 2.\n",
    "            # Caveat: When calculating the loss, you should only consider positions where valid_mask == True\n",
    "\n",
    "            # START HERE\n",
    "\n",
    "            logits = model(word_idxs,valid_mask)\n",
    "            gt = F.one_hot(tag_idxs,num_classes=logits.shape[-1])\n",
    "            loss = torch.sum(torch.abs(gt-logits))\n",
    "            # END\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # we get (unmasked) predictions by getting argmax of logits (You will need to define logits!)\n",
    "            net_predictions = torch.argmax(logits, -1)\n",
    "\n",
    "            # flattening a tensor simply converts it from a multi-dimensional to a single-dimensional tensor; we flatten here to make it easier to extract ground truths and predictions\n",
    "            tag_idxs_flat = tag_idxs.flatten()\n",
    "            valid_mask_flat = valid_mask.flatten()\n",
    "            net_predictions_flat = net_predictions.flatten()\n",
    "\n",
    "            ground_truth.extend(tag_idxs_flat[valid_mask_flat].tolist())\n",
    "            predictions.extend(net_predictions_flat[valid_mask_flat].tolist())\n",
    "\n",
    "    return np.mean(losses), eval_metrics(ground_truth, predictions)\n",
    "\n",
    "\n",
    "def train_val_loop_lstm(hyperparams: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Train and validate the LSTM model for a number of epochs.\n",
    "    \"\"\"\n",
    "    print(\"Hyperparameters:\", hyperparams)\n",
    "    # Create the dataloaders\n",
    "    loader_train, loader_val, ds_train = create_sequence_dataloaders(\n",
    "        hyperparams[\"batch_size\"]\n",
    "    )\n",
    "    # Create the model\n",
    "    model = LSTM(\n",
    "        ds_train.words_vocab,\n",
    "        ds_train.tags_vocab,\n",
    "        hyperparams[\"d_emb\"],\n",
    "        hyperparams[\"d_hidden\"],\n",
    "        hyperparams[\"bidirectional\"],\n",
    "    )\n",
    "    device = get_device()\n",
    "    model.to(device)\n",
    "    #model.to(\"cuda:0\")\n",
    "    print(model)\n",
    "    # Create the optimizer\n",
    "    optimizer = optim.RMSprop(\n",
    "        model.parameters(), hyperparams[\"learning_rate\"], weight_decay=hyperparams[\"l2\"]\n",
    "    )\n",
    "\n",
    "    # Train and validate\n",
    "    for i in range(hyperparams[\"num_epochs\"]):\n",
    "        print(\"Epoch #%d\" % i)\n",
    "\n",
    "        print(\"Training..\")\n",
    "        loss_train, metrics_train = train_lstm(model, loader_train, optimizer, device)\n",
    "        print(\"Training loss: \", loss_train)\n",
    "        print(\"Training metrics:\")\n",
    "        for k, v in metrics_train.items():\n",
    "            print(\"\\t\", k, \": \", v)\n",
    "\n",
    "        print(\"Validating..\")\n",
    "        loss_val, metrics_val = validate_lstm(model, loader_val, device)\n",
    "        print(\"Validation loss: \", loss_val)\n",
    "        print(\"Validation metrics:\")\n",
    "        for k, v in metrics_val.items():\n",
    "            print(\"\\t\", k, \": \", v)\n",
    "\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rU9Nef7yal_M"
   },
   "source": [
    "Run the experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "pFxQxlokai6Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: {'bidirectional': True, 'batch_size': 512, 'd_emb': 64, 'd_hidden': 128, 'num_epochs': 15, 'learning_rate': 0.005, 'l2': 1e-06}\n",
      "Loading data from eng.train\n",
      "14041 sentences loaded.\n",
      "Loading data from eng.val\n",
      "3490 sentences loaded.\n",
      "LSTM(\n",
      "  (E): Embedding(20102, 64)\n",
      "  (R): LSTM(64, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (act): ReLU()\n",
      "  (L): Linear(in_features=256, out_features=5, bias=True)\n",
      "  (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (S): Softmax(dim=2)\n",
      ")\n",
      "Epoch #0\n",
      "Training..\n",
      "Training loss:  25051.129557291668\n",
      "Training metrics:\n",
      "\t accuracy :  0.8091999221529694\n",
      "\t f1 :  [3.30129139e-03 8.96200919e-01 4.16233091e-04 3.76612953e-02\n",
      " 1.91731576e-03]\n",
      "\t average f1 :  0.1878994109697305\n",
      "\t confusion matrix :  [[    17   9553      9    285      3]\n",
      " [   370 161827    241   4267    142]\n",
      " [    15   4373      1    136      5]\n",
      " [    12  10633     12    305      8]\n",
      " [    18   7907     12    234      8]]\n",
      "Validating..\n",
      "Validation loss:  23844.807477678572\n",
      "Validation metrics:\n",
      "\t accuracy :  0.8386097869046164\n",
      "\t f1 :  [0.         0.91222161 0.         0.         0.        ]\n",
      "\t average f1 :  0.1824443213296399\n",
      "\t confusion matrix :  [[    0  2250     0     0     0]\n",
      " [    0 41164     0     0     0]\n",
      " [    0  1007     0     0     0]\n",
      " [    0  2690     0     0     0]\n",
      " [    0  1975     0     0     0]]\n",
      "Epoch #1\n",
      "Training..\n",
      "Training loss:  24422.78096064815\n",
      "Training metrics:\n",
      "\t accuracy :  0.8327494879933826\n",
      "\t f1 :  [0.         0.90874339 0.         0.         0.        ]\n",
      "\t average f1 :  0.18174867726306287\n",
      "\t confusion matrix :  [[     0   9847      0      0      0]\n",
      " [     0 167117      0      0      0]\n",
      " [     0   4535      0      0      0]\n",
      " [     0  10987      0      0      0]\n",
      " [     0   8195      0      0      0]]\n",
      "Validating..\n",
      "Validation loss:  23864.139787946428\n",
      "Validation metrics:\n",
      "\t accuracy :  0.8386097869046164\n",
      "\t f1 :  [0.         0.91222161 0.         0.         0.        ]\n",
      "\t average f1 :  0.1824443213296399\n",
      "\t confusion matrix :  [[    0  2250     0     0     0]\n",
      " [    0 41164     0     0     0]\n",
      " [    0  1007     0     0     0]\n",
      " [    0  2690     0     0     0]\n",
      " [    0  1975     0     0     0]]\n",
      "Epoch #2\n",
      "Training..\n",
      "Training loss:  24604.544560185186\n",
      "Training metrics:\n",
      "\t accuracy :  0.8357910185499491\n",
      "\t f1 :  [0.1088338  0.91193401 0.         0.         0.        ]\n",
      "\t average f1 :  0.20415356073244997\n",
      "\t confusion matrix :  [[   608   9279      0      0      0]\n",
      " [     3 166821      0      0      0]\n",
      " [    74   4426      0      0      0]\n",
      " [   369  10581      0      0      0]\n",
      " [   232   7931      0      0      0]]\n",
      "Validating..\n",
      "Validation loss:  23784.795758928572\n",
      "Validation metrics:\n",
      "\t accuracy :  0.8439473576987329\n",
      "\t f1 :  [0.19073756 0.91863593 0.         0.         0.        ]\n",
      "\t average f1 :  0.22187469933544338\n",
      "\t confusion matrix :  [[  278  1972     0     0     0]\n",
      " [   16 41148     0     0     0]\n",
      " [   48   959     0     0     0]\n",
      " [  185  2505     0     0     0]\n",
      " [  138  1837     0     0     0]]\n",
      "Epoch #3\n",
      "Training..\n",
      "Training loss:  24432.58716724537\n",
      "Training metrics:\n",
      "\t accuracy :  0.8492980575019325\n",
      "\t f1 :  [0.35192198 0.93216978 0.         0.         0.        ]\n",
      "\t average f1 :  0.25681835290747973\n",
      "\t confusion matrix :  [[  3392   6476      0      0      0]\n",
      " [    89 166905      0      0      0]\n",
      " [   625   3889      0      0      0]\n",
      " [  3017   7951      0      0      0]\n",
      " [  2286   5885      0      0      0]]\n",
      "Validating..\n",
      "Validation loss:  23702.571149553572\n",
      "Validation metrics:\n",
      "\t accuracy :  0.8505683901723505\n",
      "\t f1 :  [0.28471738 0.93284657 0.         0.         0.        ]\n",
      "\t average f1 :  0.24351278849988905\n",
      "\t confusion matrix :  [[  612  1638     0     0     0]\n",
      " [   25 41139     0     0     0]\n",
      " [  127   880     0     0     0]\n",
      " [  671  2019     0     0     0]\n",
      " [  614  1361     0     0     0]]\n",
      "Epoch #4\n",
      "Training..\n",
      "Training loss:  24597.030164930555\n",
      "Training metrics:\n",
      "\t accuracy :  0.8578916163875783\n",
      "\t f1 :  [0.3916878  0.95077236 0.         0.         0.        ]\n",
      "\t average f1 :  0.26849203156462187\n",
      "\t confusion matrix :  [[  5174   4719      0      0      0]\n",
      " [   124 167016      0      0      0]\n",
      " [  1353   3185      0      0      0]\n",
      " [  5398   5559      0      0      0]\n",
      " [  4477   3708      0      0      0]]\n",
      "Validating..\n",
      "Validation loss:  23625.68359375\n",
      "Validation metrics:\n",
      "\t accuracy :  0.8608972008311943\n",
      "\t f1 :  [0.37117003 0.95333001 0.         0.         0.        ]\n",
      "\t average f1 :  0.2649000069521885\n",
      "\t confusion matrix :  [[ 1169  1081     0     0     0]\n",
      " [   75 41089     0     0     0]\n",
      " [  286   721     0     0     0]\n",
      " [ 1231  1459     0     0     0]\n",
      " [ 1288   687     0     0     0]]\n",
      "Epoch #5\n",
      "Training..\n",
      "Training loss:  24163.42896412037\n",
      "Training metrics:\n",
      "\t accuracy :  0.8632322919734261\n",
      "\t f1 :  [0.40253627 0.96402707 0.         0.         0.        ]\n",
      "\t average f1 :  0.27331266749856814\n",
      "\t confusion matrix :  [[  6285   3560      0      0      0]\n",
      " [   134 166661      0      0      0]\n",
      " [  1920   2610      0      0      0]\n",
      " [  7290   3707      0      0      0]\n",
      " [  5753   2427      0      0      0]]\n",
      "Validating..\n",
      "Validation loss:  23608.023995535714\n",
      "Validation metrics:\n",
      "\t accuracy :  0.8632807725216967\n",
      "\t f1 :  [0.37712652 0.96031811 0.         0.         0.        ]\n",
      "\t average f1 :  0.26748892579882905\n",
      "\t confusion matrix :  [[ 1319   931     0     0     0]\n",
      " [  108 41056     0     0     0]\n",
      " [  361   646     0     0     0]\n",
      " [ 1509  1181     0     0     0]\n",
      " [ 1448   527     0     0     0]]\n",
      "Epoch #6\n",
      "Training..\n",
      "Training loss:  24447.84403935185\n",
      "Training metrics:\n",
      "\t accuracy :  0.8663711651700463\n",
      "\t f1 :  [0.41077381 0.97051707 0.         0.         0.        ]\n",
      "\t average f1 :  0.2762581760849515\n",
      "\t confusion matrix :  [[  6901   3007      0      0      0]\n",
      " [   128 166861      0      0      0]\n",
      " [  2265   2271      0      0      0]\n",
      " [  8128   2849      0      0      0]\n",
      " [  6270   1883      0      0      0]]\n",
      "Validating..\n",
      "Validation loss:  23591.186941964286\n",
      "Validation metrics:\n",
      "\t accuracy :  0.8636882206739193\n",
      "\t f1 :  [0.37431615 0.9606798  0.         0.         0.        ]\n",
      "\t average f1 :  0.2669991915499125\n",
      "\t confusion matrix :  [[ 1300   950     0     0     0]\n",
      " [   69 41095     0     0     0]\n",
      " [  364   643     0     0     0]\n",
      " [ 1482  1208     0     0     0]\n",
      " [ 1481   494     0     0     0]]\n",
      "Epoch #7\n",
      "Training..\n",
      "Training loss:  24272.49580439815\n",
      "Training metrics:\n",
      "\t accuracy :  0.8679520786197685\n",
      "\t f1 :  [0.41271309 0.97374742 0.         0.         0.        ]\n",
      "\t average f1 :  0.27729210178377284\n",
      "\t confusion matrix :  [[  7142   2714      0      0      0]\n",
      " [   121 167023      0      0      0]\n",
      " [  2399   2122      0      0      0]\n",
      " [  8572   2409      0      0      0]\n",
      " [  6520   1640      0      0      0]]\n",
      "Validating..\n",
      "Validation loss:  23586.080636160714\n",
      "Validation metrics:\n",
      "\t accuracy :  0.8645031169783645\n",
      "\t f1 :  [0.37702905 0.96403621 0.         0.         0.        ]\n",
      "\t average f1 :  0.2682130530114903\n",
      "\t confusion matrix :  [[ 1382   868     0     0     0]\n",
      " [  111 41053     0     0     0]\n",
      " [  408   599     0     0     0]\n",
      " [ 1625  1065     0     0     0]\n",
      " [ 1555   420     0     0     0]]\n",
      "Epoch #8\n",
      "Training..\n",
      "Training loss:  24102.732277199073\n",
      "Training metrics:\n",
      "\t accuracy :  0.8689937803867541\n",
      "\t f1 :  [0.41686117 0.97625219 0.         0.         0.        ]\n",
      "\t average f1 :  0.27862267071654234\n",
      "\t confusion matrix :  [[  7412   2481      0      0      0]\n",
      " [   120 166677      0      0      0]\n",
      " [  2564   1963      0      0      0]\n",
      " [  8918   2050      0      0      0]\n",
      " [  6654   1495      0      0      0]]\n",
      "Validating..\n",
      "Validation loss:  23580.03738839286\n",
      "Validation metrics:\n",
      "\t accuracy :  0.864890192722976\n",
      "\t f1 :  [0.37940896 0.96435995 0.         0.         0.        ]\n",
      "\t average f1 :  0.26875378156734514\n",
      "\t confusion matrix :  [[ 1393   857     0     0     0]\n",
      " [  103 41061     0     0     0]\n",
      " [  410   597     0     0     0]\n",
      " [ 1623  1067     0     0     0]\n",
      " [ 1564   411     0     0     0]]\n",
      "Epoch #9\n",
      "Training..\n",
      "Training loss:  24667.516059027777\n",
      "Training metrics:\n",
      "\t accuracy :  0.870215319941625\n",
      "\t f1 :  [0.42156754 0.9780149  0.         0.         0.        ]\n",
      "\t average f1 :  0.2799164892474151\n",
      "\t confusion matrix :  [[  7627   2266      0      0      0]\n",
      " [   119 167087      0      0      0]\n",
      " [  2638   1872      0      0      0]\n",
      " [  9120   1849      0      0      0]\n",
      " [  6787   1406      0      0      0]]\n",
      "Validating..\n",
      "Validation loss:  23587.161272321428\n",
      "Validation metrics:\n",
      "\t accuracy :  0.865053171983865\n",
      "\t f1 :  [0.37969037 0.96644596 0.         0.         0.        ]\n",
      "\t average f1 :  0.2692272655418388\n",
      "\t confusion matrix :  [[ 1447   803     0     0     0]\n",
      " [  149 41015     0     0     0]\n",
      " [  425   582     0     0     0]\n",
      " [ 1749   941     0     0     0]\n",
      " [ 1602   373     0     0     0]]\n",
      "Epoch #10\n",
      "Training..\n",
      "Training loss:  24555.530164930555\n",
      "Training metrics:\n",
      "\t accuracy :  0.8706550518825823\n",
      "\t f1 :  [0.4229145 0.9789428 0.        0.        0.       ]\n",
      "\t average f1 :  0.2803714590375678\n",
      "\t confusion matrix :  [[  7711   2178      0      0      0]\n",
      " [   106 166898      0      0      0]\n",
      " [  2704   1810      0      0      0]\n",
      " [  9232   1734      0      0      0]\n",
      " [  6824   1352      0      0      0]]\n",
      "Validating..\n",
      "Validation loss:  23573.292131696428\n",
      "Validation metrics:\n",
      "\t accuracy :  0.8653995029132543\n",
      "\t f1 :  [0.38029478 0.96616413 0.         0.         0.        ]\n",
      "\t average f1 :  0.26929178240341756\n",
      "\t confusion matrix :  [[ 1432   818     0     0     0]\n",
      " [  117 41047     0     0     0]\n",
      " [  436   571     0     0     0]\n",
      " [ 1700   990     0     0     0]\n",
      " [ 1596   379     0     0     0]]\n",
      "Epoch #11\n",
      "Training..\n",
      "Training loss:  24014.52980324074\n",
      "Training metrics:\n",
      "\t accuracy :  0.8712289987689578\n",
      "\t f1 :  [0.42320543 0.9806862  0.         0.         0.        ]\n",
      "\t average f1 :  0.2807783271762364\n",
      "\t confusion matrix :  [[  7853   2036      0      0      0]\n",
      " [   109 166953      0      0      0]\n",
      " [  2809   1722      0      0      0]\n",
      " [  9469   1507      0      0      0]\n",
      " [  6983   1202      0      0      0]]\n",
      "Validating..\n",
      "Validation loss:  23567.200055803572\n",
      "Validation metrics:\n",
      "\t accuracy :  0.8655828545817544\n",
      "\t f1 :  [0.37863691 0.96649524 0.         0.         0.        ]\n",
      "\t average f1 :  0.2690264293597361\n",
      "\t confusion matrix :  [[ 1425   825     0     0     0]\n",
      " [  101 41063     0     0     0]\n",
      " [  441   566     0     0     0]\n",
      " [ 1709   981     0     0     0]\n",
      " [ 1601   374     0     0     0]]\n",
      "Epoch #12\n",
      "Training..\n",
      "Training loss:  23935.12919560185\n",
      "Training metrics:\n",
      "\t accuracy :  0.8722864744258482\n",
      "\t f1 :  [0.42587529 0.98176371 0.         0.         0.        ]\n",
      "\t average f1 :  0.28152780043746123\n",
      "\t confusion matrix :  [[  7943   1915      0      0      0]\n",
      " [   112 166810      0      0      0]\n",
      " [  2864   1641      0      0      0]\n",
      " [  9501   1399      0      0      0]\n",
      " [  7024   1130      0      0      0]]\n",
      "Validating..\n",
      "Validation loss:  23569.237165178572\n",
      "Validation metrics:\n",
      "\t accuracy :  0.8658884406959214\n",
      "\t f1 :  [0.38222684 0.96770161 0.         0.         0.        ]\n",
      "\t average f1 :  0.26998569119125787\n",
      "\t confusion matrix :  [[ 1471   779     0     0     0]\n",
      " [  132 41032     0     0     0]\n",
      " [  450   557     0     0     0]\n",
      " [ 1768   922     0     0     0]\n",
      " [ 1626   349     0     0     0]]\n",
      "Epoch #13\n",
      "Training..\n",
      "Training loss:  24294.01070601852\n",
      "Training metrics:\n",
      "\t accuracy :  0.8722116108697929\n",
      "\t f1 :  [0.4271721  0.98238642 0.         0.         0.        ]\n",
      "\t average f1 :  0.2819117021471014\n",
      "\t confusion matrix :  [[  8046   1829      0      0      0]\n",
      " [   109 166849      0      0      0]\n",
      " [  2924   1606      0      0      0]\n",
      " [  9618   1350      0      0      0]\n",
      " [  7099   1089      0      0      0]]\n",
      "Validating..\n",
      "Validation loss:  23561.048270089286\n",
      "Validation metrics:\n",
      "\t accuracy :  0.866132909587255\n",
      "\t f1 :  [0.3805135  0.96743419 0.         0.         0.        ]\n",
      "\t average f1 :  0.26958953737513786\n",
      "\t confusion matrix :  [[ 1445   805     0     0     0]\n",
      " [   94 41070     0     0     0]\n",
      " [  456   551     0     0     0]\n",
      " [ 1722   968     0     0     0]\n",
      " [ 1628   347     0     0     0]]\n",
      "Epoch #14\n",
      "Training..\n",
      "Training loss:  24414.61451099537\n",
      "Training metrics:\n",
      "\t accuracy :  0.8724111058936368\n",
      "\t f1 :  [0.42779645 0.98291562 0.         0.         0.        ]\n",
      "\t average f1 :  0.28214241308433674\n",
      "\t confusion matrix :  [[  8083   1768      0      0      0]\n",
      " [   104 166558      0      0      0]\n",
      " [  2942   1582      0      0      0]\n",
      " [  9672   1296      0      0      0]\n",
      " [  7137   1040      0      0      0]]\n",
      "Validating..\n",
      "Validation loss:  23564.301897321428\n",
      "Validation metrics:\n",
      "\t accuracy :  0.8665811025546999\n",
      "\t f1 :  [0.38465473 0.96913085 0.         0.         0.        ]\n",
      "\t average f1 :  0.2707571153987873\n",
      "\t confusion matrix :  [[ 1504   746     0     0     0]\n",
      " [  131 41033     0     0     0]\n",
      " [  465   542     0     0     0]\n",
      " [ 1822   868     0     0     0]\n",
      " [ 1648   327     0     0     0]]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_val_loop_lstm({\n",
    "    \"bidirectional\": True,\n",
    "    \"batch_size\": 512,\n",
    "    \"d_emb\": 64,\n",
    "    \"d_hidden\": 128,\n",
    "    \"num_epochs\": 15,\n",
    "    \"learning_rate\": 0.005,\n",
    "    \"l2\": 1e-6,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vA-Yjqg7n0V"
   },
   "source": [
    "We were using bidirectional LSTMs. Please re-run the experiment with a regular (unidirectional) LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "7wNrdvJ98ARB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: {'bidirectional': False, 'batch_size': 512, 'd_emb': 64, 'd_hidden': 128, 'num_epochs': 15, 'learning_rate': 0.005, 'l2': 1e-06}\n",
      "Loading data from eng.train\n",
      "14041 sentences loaded.\n",
      "Loading data from eng.val\n",
      "3490 sentences loaded.\n",
      "LSTM(\n",
      "  (E): Embedding(20102, 64)\n",
      "  (R): LSTM(64, 128, num_layers=2, batch_first=True)\n",
      "  (act): ReLU()\n",
      "  (L): Linear(in_features=128, out_features=5, bias=True)\n",
      "  (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (S): Softmax(dim=2)\n",
      ")\n",
      "Epoch #0\n",
      "Training..\n",
      "Training loss:  24908.03399884259\n",
      "Training metrics:\n",
      "\t accuracy :  0.8043718138833551\n",
      "\t f1 :  [0.         0.89422746 0.02049587 0.         0.02756593]\n",
      "\t average f1 :  0.18845785028940068\n",
      "\t confusion matrix :  [[     0   9450    267      0    146]\n",
      " [     0 160859   3713      1   2213]\n",
      " [     0   4353     93      0     49]\n",
      " [     0  10508    306      0    158]\n",
      " [     0   7816    201      0    150]]\n",
      "Validating..\n",
      "Validation loss:  23849.491350446428\n",
      "Validation metrics:\n",
      "\t accuracy :  0.8386097869046164\n",
      "\t f1 :  [0.         0.91222161 0.         0.         0.        ]\n",
      "\t average f1 :  0.1824443213296399\n",
      "\t confusion matrix :  [[    0  2250     0     0     0]\n",
      " [    0 41164     0     0     0]\n",
      " [    0  1007     0     0     0]\n",
      " [    0  2690     0     0     0]\n",
      " [    0  1975     0     0     0]]\n",
      "Epoch #1\n",
      "Training..\n",
      "Training loss:  24875.01048900463\n",
      "Training metrics:\n",
      "\t accuracy :  0.832812960529931\n",
      "\t f1 :  [0.         0.90878118 0.         0.         0.        ]\n",
      "\t average f1 :  0.18175623557117043\n",
      "\t confusion matrix :  [[     0   9837      0      0      0]\n",
      " [     0 166710      0      0      0]\n",
      " [     0   4516      0      0      0]\n",
      " [     0  10947      0      0      0]\n",
      " [     0   8167      0      0      0]]\n",
      "Validating..\n",
      "Validation loss:  23840.20089285714\n",
      "Validation metrics:\n",
      "\t accuracy :  0.8386097869046164\n",
      "\t f1 :  [0.         0.91222161 0.         0.         0.        ]\n",
      "\t average f1 :  0.1824443213296399\n",
      "\t confusion matrix :  [[    0  2250     0     0     0]\n",
      " [    0 41164     0     0     0]\n",
      " [    0  1007     0     0     0]\n",
      " [    0  2690     0     0     0]\n",
      " [    0  1975     0     0     0]]\n",
      "Epoch #2\n",
      "Training..\n",
      "Training loss:  24866.380859375\n",
      "Training metrics:\n",
      "\t accuracy :  0.8327673981441315\n",
      "\t f1 :  [0.         0.90875405 0.         0.         0.        ]\n",
      "\t average f1 :  0.18175081005639787\n",
      "\t confusion matrix :  [[     0   9875      0      0      0]\n",
      " [     0 167014      0      0      0]\n",
      " [     0   4521      0      0      0]\n",
      " [     0  10975      0      0      0]\n",
      " [     0   8168      0      0      0]]\n",
      "Validating..\n",
      "Validation loss:  23840.080078125\n",
      "Validation metrics:\n",
      "\t accuracy :  0.8386097869046164\n",
      "\t f1 :  [0.         0.91222161 0.         0.         0.        ]\n",
      "\t average f1 :  0.1824443213296399\n",
      "\t confusion matrix :  [[    0  2250     0     0     0]\n",
      " [    0 41164     0     0     0]\n",
      " [    0  1007     0     0     0]\n",
      " [    0  2690     0     0     0]\n",
      " [    0  1975     0     0     0]]\n",
      "Epoch #3\n",
      "Training..\n",
      "Training loss:  24540.32718460648\n",
      "Training metrics:\n",
      "\t accuracy :  0.8329685560302923\n",
      "\t f1 :  [0.         0.90887381 0.         0.         0.        ]\n",
      "\t average f1 :  0.18177476166514803\n",
      "\t confusion matrix :  [[     0   9912      0      0      0]\n",
      " [     0 167076      0      0      0]\n",
      " [     0   4510      0      0      0]\n",
      " [     0  10935      0      0      0]\n",
      " [     0   8146      0      0      0]]\n",
      "Validating..\n",
      "Validation loss:  23840.04268973214\n",
      "Validation metrics:\n",
      "\t accuracy :  0.8386097869046164\n",
      "\t f1 :  [0.         0.91222161 0.         0.         0.        ]\n",
      "\t average f1 :  0.1824443213296399\n",
      "\t confusion matrix :  [[    0  2250     0     0     0]\n",
      " [    0 41164     0     0     0]\n",
      " [    0  1007     0     0     0]\n",
      " [    0  2690     0     0     0]\n",
      " [    0  1975     0     0     0]]\n",
      "Epoch #4\n",
      "Training..\n",
      "Training loss:  24713.26417824074\n",
      "Training metrics:\n",
      "\t accuracy :  0.8328903472246463\n",
      "\t f1 :  [0.         0.90882725 0.         0.         0.        ]\n",
      "\t average f1 :  0.1817654500686972\n",
      "\t confusion matrix :  [[     0   9879      0      0      0]\n",
      " [     0 167022      0      0      0]\n",
      " [     0   4517      0      0      0]\n",
      " [     0  10947      0      0      0]\n",
      " [     0   8168      0      0      0]]\n",
      "Validating..\n",
      "Validation loss:  23840.034598214286\n",
      "Validation metrics:\n",
      "\t accuracy :  0.8386097869046164\n",
      "\t f1 :  [0.         0.91222161 0.         0.         0.        ]\n",
      "\t average f1 :  0.1824443213296399\n",
      "\t confusion matrix :  [[    0  2250     0     0     0]\n",
      " [    0 41164     0     0     0]\n",
      " [    0  1007     0     0     0]\n",
      " [    0  2690     0     0     0]\n",
      " [    0  1975     0     0     0]]\n",
      "Epoch #5\n",
      "Training..\n",
      "Training loss:  24464.551215277777\n",
      "Training metrics:\n",
      "\t accuracy :  0.8329112788251598\n",
      "\t f1 :  [0.         0.90883971 0.         0.         0.        ]\n",
      "\t average f1 :  0.18176794227793297\n",
      "\t confusion matrix :  [[     0   9884      0      0      0]\n",
      " [     0 167087      0      0      0]\n",
      " [     0   4523      0      0      0]\n",
      " [     0  10948      0      0      0]\n",
      " [     0   8164      0      0      0]]\n",
      "Validating..\n",
      "Validation loss:  23839.988839285714\n",
      "Validation metrics:\n",
      "\t accuracy :  0.8386097869046164\n",
      "\t f1 :  [0.         0.91222161 0.         0.         0.        ]\n",
      "\t average f1 :  0.1824443213296399\n",
      "\t confusion matrix :  [[    0  2250     0     0     0]\n",
      " [    0 41164     0     0     0]\n",
      " [    0  1007     0     0     0]\n",
      " [    0  2690     0     0     0]\n",
      " [    0  1975     0     0     0]]\n",
      "Epoch #6\n",
      "Training..\n",
      "Training loss:  24695.412471064814\n",
      "Training metrics:\n",
      "\t accuracy :  0.8328668153536805\n",
      "\t f1 :  [0.         0.90881324 0.         0.         0.        ]\n",
      "\t average f1 :  0.18176264819175433\n",
      "\t confusion matrix :  [[     0   9879      0      0      0]\n",
      " [     0 166924      0      0      0]\n",
      " [     0   4519      0      0      0]\n",
      " [     0  10914      0      0      0]\n",
      " [     0   8185      0      0      0]]\n",
      "Validating..\n",
      "Validation loss:  23837.159319196428\n",
      "Validation metrics:\n",
      "\t accuracy :  0.8386097869046164\n",
      "\t f1 :  [0.         0.91222161 0.         0.         0.        ]\n",
      "\t average f1 :  0.1824443213296399\n",
      "\t confusion matrix :  [[    0  2250     0     0     0]\n",
      " [    0 41164     0     0     0]\n",
      " [    0  1007     0     0     0]\n",
      " [    0  2690     0     0     0]\n",
      " [    0  1975     0     0     0]]\n",
      "Epoch #7\n",
      "Training..\n",
      "Training loss:  24714.414641203704\n",
      "Training metrics:\n",
      "\t accuracy :  0.8365505602268869\n",
      "\t f1 :  [0.13189119 0.91471397 0.         0.         0.        ]\n",
      "\t average f1 :  0.20932103210409428\n",
      "\t confusion matrix :  [[   817   9068      0      0      0]\n",
      " [    42 166724      0      0      0]\n",
      " [    89   4406      0      0      0]\n",
      " [  1165   9772      0      0      0]\n",
      " [   391   7802      0      0      0]]\n",
      "Validating..\n",
      "Validation loss:  23778.112723214286\n",
      "Validation metrics:\n",
      "\t accuracy :  0.8438454956606771\n",
      "\t f1 :  [0.17185094 0.92191859 0.         0.         0.        ]\n",
      "\t average f1 :  0.2187539060600686\n",
      "\t confusion matrix :  [[  279  1971     0     0     0]\n",
      " [   22 41142     0     0     0]\n",
      " [   33   974     0     0     0]\n",
      " [  451  2239     0     0     0]\n",
      " [  212  1763     0     0     0]]\n",
      "Epoch #8\n",
      "Training..\n",
      "Training loss:  24561.407624421296\n",
      "Training metrics:\n",
      "\t accuracy :  0.847458304204534\n",
      "\t f1 :  [0.29351301 0.93504308 0.         0.         0.        ]\n",
      "\t average f1 :  0.24571121867314222\n",
      "\t confusion matrix :  [[  2984   6852      0      0      0]\n",
      " [   130 166728      0      0      0]\n",
      " [   631   3900      0      0      0]\n",
      " [  4223   6712      0      0      0]\n",
      " [  2529   5571      0      0      0]]\n",
      "Validating..\n",
      "Validation loss:  23684.326171875\n",
      "Validation metrics:\n",
      "\t accuracy :  0.8513425416615735\n",
      "\t f1 :  [0.26508408 0.940454   0.         0.         0.        ]\n",
      "\t average f1 :  0.24110761496600466\n",
      "\t confusion matrix :  [[  670  1580     0     0     0]\n",
      " [   45 41119     0     0     0]\n",
      " [  157   850     0     0     0]\n",
      " [ 1032  1658     0     0     0]\n",
      " [  901  1074     0     0     0]]\n",
      "Epoch #9\n",
      "Training..\n",
      "Training loss:  23860.06155960648\n",
      "Training metrics:\n",
      "\t accuracy :  0.854780034684155\n",
      "\t f1 :  [0.34056661 0.95128974 0.         0.         0.        ]\n",
      "\t average f1 :  0.25837126948317735\n",
      "\t confusion matrix :  [[  4520   5308      0      0      0]\n",
      " [   158 167007      0      0      0]\n",
      " [  1354   3167      0      0      0]\n",
      " [  6192   4778      0      0      0]\n",
      " [  4492   3692      0      0      0]]\n",
      "Validating..\n",
      "Validation loss:  23640.809151785714\n",
      "Validation metrics:\n",
      "\t accuracy :  0.8554781404066333\n",
      "\t f1 :  [0.2954955  0.95121245 0.         0.         0.        ]\n",
      "\t average f1 :  0.24934158998398828\n",
      "\t confusion matrix :  [[  902  1348     0     0     0]\n",
      " [   74 41090     0     0     0]\n",
      " [  251   756     0     0     0]\n",
      " [ 1457  1233     0     0     0]\n",
      " [ 1171   804     0     0     0]]\n",
      "Epoch #10\n",
      "Training..\n",
      "Training loss:  24002.812861689814\n",
      "Training metrics:\n",
      "\t accuracy :  0.8584623065401897\n",
      "\t f1 :  [0.36003242 0.95952156 0.         0.         0.        ]\n",
      "\t average f1 :  0.2639107955917538\n",
      "\t confusion matrix :  [[  5331   4503      0      0      0]\n",
      " [   157 166619      0      0      0]\n",
      " [  1670   2853      0      0      0]\n",
      " [  7389   3594      0      0      0]\n",
      " [  5233   2951      0      0      0]]\n",
      "Validating..\n",
      "Validation loss:  23617.614118303572\n",
      "Validation metrics:\n",
      "\t accuracy :  0.8574746363525242\n",
      "\t f1 :  [0.30693526 0.95566124 0.         0.         0.        ]\n",
      "\t average f1 :  0.25251930092916297\n",
      "\t confusion matrix :  [[  998  1252     0     0     0]\n",
      " [   72 41092     0     0     0]\n",
      " [  300   707     0     0     0]\n",
      " [ 1585  1105     0     0     0]\n",
      " [ 1298   677     0     0     0]]\n",
      "Epoch #11\n",
      "Training..\n",
      "Training loss:  24290.062138310186\n",
      "Training metrics:\n",
      "\t accuracy :  0.8607135738489137\n",
      "\t f1 :  [0.36798675 0.96435781 0.         0.         0.        ]\n",
      "\t average f1 :  0.26646891200711864\n",
      "\t confusion matrix :  [[  5775   4086      0      0      0]\n",
      " [   140 166953      0      0      0]\n",
      " [  1862   2668      0      0      0]\n",
      " [  8081   2931      0      0      0]\n",
      " [  5668   2516      0      0      0]]\n",
      "Validating..\n",
      "Validation loss:  23611.33761160714\n",
      "Validation metrics:\n",
      "\t accuracy :  0.8583506498798028\n",
      "\t f1 :  [0.31385643 0.95678085 0.         0.         0.        ]\n",
      "\t average f1 :  0.25412745639686246\n",
      "\t confusion matrix :  [[ 1034  1216     0     0     0]\n",
      " [   65 41099     0     0     0]\n",
      " [  317   690     0     0     0]\n",
      " [ 1587  1103     0     0     0]\n",
      " [ 1336   639     0     0     0]]\n",
      "Epoch #12\n",
      "Training..\n",
      "Training loss:  24102.393952546296\n",
      "Training metrics:\n",
      "\t accuracy :  0.8622471607755978\n",
      "\t f1 :  [0.37347917 0.96756527 0.         0.         0.        ]\n",
      "\t average f1 :  0.2682088868485011\n",
      "\t confusion matrix :  [[  6078   3801      0      0      0]\n",
      " [   141 166950      0      0      0]\n",
      " [  1993   2529      0      0      0]\n",
      " [  8433   2567      0      0      0]\n",
      " [  6024   2155      0      0      0]]\n",
      "Validating..\n",
      "Validation loss:  23605.44363839286\n",
      "Validation metrics:\n",
      "\t accuracy :  0.8592470358146925\n",
      "\t f1 :  [0.31953522 0.95957484 0.         0.         0.        ]\n",
      "\t average f1 :  0.25582201247066644\n",
      "\t confusion matrix :  [[ 1100  1150     0     0     0]\n",
      " [   87 41077     0     0     0]\n",
      " [  334   673     0     0     0]\n",
      " [ 1710   980     0     0     0]\n",
      " [ 1404   571     0     0     0]]\n",
      "Epoch #13\n",
      "Training..\n",
      "Training loss:  24129.559678819445\n",
      "Training metrics:\n",
      "\t accuracy :  0.8633721799639115\n",
      "\t f1 :  [0.37804915 0.96949054 0.         0.         0.        ]\n",
      "\t average f1 :  0.26950793786158317\n",
      "\t confusion matrix :  [[  6269   3585      0      0      0]\n",
      " [   139 166939      0      0      0]\n",
      " [  2079   2447      0      0      0]\n",
      " [  8645   2349      0      0      0]\n",
      " [  6179   1987      0      0      0]]\n",
      "Validating..\n",
      "Validation loss:  23614.75418526786\n",
      "Validation metrics:\n",
      "\t accuracy :  0.8599600700810822\n",
      "\t f1 :  [0.32431671 0.96159341 0.         0.         0.        ]\n",
      "\t average f1 :  0.25718202284775216\n",
      "\t confusion matrix :  [[ 1151  1099     0     0     0]\n",
      " [  103 41061     0     0     0]\n",
      " [  349   658     0     0     0]\n",
      " [ 1817   873     0     0     0]\n",
      " [ 1428   547     0     0     0]]\n",
      "Epoch #14\n",
      "Training..\n",
      "Training loss:  24058.97439236111\n",
      "Training metrics:\n",
      "\t accuracy :  0.8644668761547157\n",
      "\t f1 :  [0.38412004 0.97130085 0.         0.         0.        ]\n",
      "\t average f1 :  0.2710841789540427\n",
      "\t confusion matrix :  [[  6502   3368      0      0      0]\n",
      " [   140 167089      0      0      0]\n",
      " [  2163   2367      0      0      0]\n",
      " [  8817   2167      0      0      0]\n",
      " [  6362   1832      0      0      0]]\n",
      "Validating..\n",
      "Validation loss:  23615.2890625\n",
      "Validation metrics:\n",
      "\t accuracy :  0.8608360836083608\n",
      "\t f1 :  [0.33231368 0.96262953 0.         0.         0.        ]\n",
      "\t average f1 :  0.2589886425021305\n",
      "\t confusion matrix :  [[ 1195  1055     0     0     0]\n",
      " [  104 41060     0     0     0]\n",
      " [  357   650     0     0     0]\n",
      " [ 1820   870     0     0     0]\n",
      " [ 1466   509     0     0     0]]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "## TODO: Re-run with unidirectional LSTMs\n",
    "## Keep other hyperparameters fixed\n",
    "train_val_loop_lstm({\n",
    "    \"bidirectional\": False,\n",
    "    \"batch_size\": 512,\n",
    "    \"d_emb\": 64,\n",
    "    \"d_hidden\": 128,\n",
    "    \"num_epochs\": 15,\n",
    "    \"learning_rate\": 0.005,\n",
    "    \"l2\": 1e-6,\n",
    "})\n",
    "## END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8UChDyKaPBs"
   },
   "source": [
    "### Questions **(2 points)**\n",
    "\n",
    "How does bidirectional LSTMs compare to unidirectional LSTMs? Why?\n",
    "\n",
    "**TODO: Please fill in your answer here**\n",
    "\n",
    "Bidirectional LSTMs showed better performance (86.6 % vs 86.0% on validation). \n",
    "Unidirectional LSTM can only use forwarding sequences. However, with bidirectional LSTMs the model can exploit contextual dependency on the backward of a sentence. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
